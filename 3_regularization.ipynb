{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add L2 loss to a logistic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    0.01 * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at step 0: 45.555367\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 16.6%\n",
      "\n",
      "Minibatch loss at step 500: 1.026521\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "\n",
      "Minibatch loss at step 1000: 0.656263\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.6%\n",
      "\n",
      "Minibatch loss at step 1500: 0.896476\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.3%\n",
      "\n",
      "Minibatch loss at step 2000: 0.792826\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.0%\n",
      "\n",
      "Minibatch loss at step 2500: 0.693669\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.2%\n",
      "\n",
      "Minibatch loss at step 3000: 0.696768\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"\\nMinibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add L2 loss to a neural network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Hidden layer\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, 1024]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([1024]))\n",
    "\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([1024, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden_layer, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    0.01 * (tf.nn.l2_loss(hidden_weights) + tf.nn.l2_loss(hidden_biases) +\n",
    "            tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at step 0: 3455.434082\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 36.9%\n",
      "\n",
      "Minibatch loss at step 500: 21.311453\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.4%\n",
      "\n",
      "Minibatch loss at step 1000: 0.827859\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.8%\n",
      "\n",
      "Minibatch loss at step 1500: 0.789544\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.4%\n",
      "\n",
      "Minibatch loss at step 2000: 0.751156\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.2%\n",
      "\n",
      "Minibatch loss at step 2500: 0.694164\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.0%\n",
      "\n",
      "Minibatch loss at step 3000: 0.677404\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.0%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"\\nMinibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Hidden layer\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, 1024]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([1024]))\n",
    "\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([1024, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(hidden_layer, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at step 0: 329.965393\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 33.7%\n",
      "\n",
      "Minibatch loss at step 500: 18.953644\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 79.2%\n",
      "\n",
      "Minibatch loss at step 1000: 3.086717\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 81.3%\n",
      "\n",
      "Minibatch loss at step 1500: 2.930993\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 82.2%\n",
      "\n",
      "Minibatch loss at step 2000: 0.028172\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 82.3%\n",
      "\n",
      "Minibatch loss at step 2500: 0.628493\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 82.1%\n",
      "\n",
      "Minibatch loss at step 3000: 0.112169\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % 10000\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"\\nMinibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network memorizes the training set and performs poorly on the validation set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Hidden layer\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, 1024]))\n",
    "  hidden_biases = tf.Variable(tf.zeros([1024]))\n",
    "\n",
    "  hidden_layer = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases), 0.5)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([1024, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.nn.dropout(tf.matmul(hidden_layer, weights) + biases, 0.5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases), weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases), weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at step 0: 759.539001\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 18.9%\n",
      "\n",
      "Minibatch loss at step 500: 65.236465\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 80.8%\n",
      "\n",
      "Minibatch loss at step 1000: 24.334694\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 80.0%\n",
      "\n",
      "Minibatch loss at step 1500: 36.486465\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 80.2%\n",
      "\n",
      "Minibatch loss at step 2000: 7.372968\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 79.7%\n",
      "\n",
      "Minibatch loss at step 2500: 18.245176\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 80.3%\n",
      "\n",
      "Minibatch loss at step 3000: 14.671097\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 86.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % 10000\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"\\nMinibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It no longer overfits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Hidden layer 1\n",
    "    hidden_weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, 1024], stddev=0.1))\n",
    "    hidden_biases1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "\n",
    "    hidden_layer1 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights1) + hidden_biases1), 0.75)\n",
    "  \n",
    "\n",
    "    # Hidden layer 2\n",
    "    hidden_weights2 = tf.Variable(\n",
    "      tf.truncated_normal([1024, 2048], stddev=0.1))\n",
    "    hidden_biases2 = tf.Variable(tf.constant(0.1, shape=[2048]))\n",
    "\n",
    "    hidden_layer2 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden_layer1, hidden_weights2) + hidden_biases2), 0.75)\n",
    "\n",
    "    # Hidden layer 3\n",
    "    hidden_weights3 = tf.Variable(\n",
    "      tf.truncated_normal([2048, 1024], stddev=0.1))\n",
    "    hidden_biases3 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "\n",
    "    hidden_layer3 = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden_layer2, hidden_weights3) + hidden_biases3), 0.75)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([1024, num_labels], stddev=0.1))\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(hidden_layer3, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.01, global_step, 100000, 0.5)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(\n",
    "          tf.nn.relu(tf.matmul(\n",
    "          tf.nn.relu(tf.matmul(\n",
    "          tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights1) + hidden_biases1)\n",
    "          , hidden_weights2) + hidden_biases2)\n",
    "          , hidden_weights3) + hidden_biases3)\n",
    "          , weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(\n",
    "          tf.nn.relu(tf.matmul(\n",
    "          tf.nn.relu(tf.matmul(\n",
    "          tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights1) + hidden_biases1)\n",
    "          , hidden_weights2) + hidden_biases2)\n",
    "          , hidden_weights3) + hidden_biases3)\n",
    "          , weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at step 0: 38.665863\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 21.6%\n",
      "[[  1.22797508e-10   8.84943452e-10   2.24076544e-08 ...,   9.64072708e-04\n",
      "    4.83818186e-09   1.08507893e-05]\n",
      " [  1.24976585e-10   5.18571075e-10   2.81640244e-13 ...,   8.74882335e-06\n",
      "    1.24464545e-08   9.99952197e-01]\n",
      " [  1.05357736e-12   6.50306864e-10   4.58491286e-07 ...,   3.07324044e-05\n",
      "    2.89439413e-07   5.59558392e-01]\n",
      " ..., \n",
      " [  2.55114656e-15   4.73291895e-09   8.64278675e-08 ...,   8.99463348e-09\n",
      "    1.29276188e-04   9.85556424e-01]\n",
      " [  3.89379883e-11   1.22651272e-03   1.51320705e-07 ...,   7.20448152e-04\n",
      "    6.47726210e-08   5.82729044e-05]\n",
      " [  1.42863943e-24   4.57323807e-07   1.03078709e-11 ...,   1.74387805e-12\n",
      "    9.99953747e-01   1.47376795e-08]]\n",
      "\n",
      "Minibatch loss at step 500: 1.320636\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 81.5%\n",
      "[[  1.63055503e-07   9.99975204e-01   1.57145408e-08 ...,   1.94403196e-08\n",
      "    1.15420233e-08   1.34765983e-07]\n",
      " [  2.75433739e-03   1.77834649e-04   8.62522138e-05 ...,   1.70343228e-05\n",
      "    3.13321277e-02   9.65437591e-01]\n",
      " [  2.36245152e-03   1.49328774e-02   6.18326478e-02 ...,   6.13547803e-04\n",
      "    7.91664898e-01   1.67877115e-02]\n",
      " ..., \n",
      " [  5.81716094e-03   5.17950300e-03   1.24928681e-02 ...,   1.61377210e-02\n",
      "    7.15188086e-01   1.55625731e-01]\n",
      " [  2.02405062e-08   2.00052974e-09   2.27816334e-12 ...,   1.23202293e-09\n",
      "    7.82944110e-09   1.00000000e+00]\n",
      " [  1.20916027e-07   1.79818560e-08   1.59545252e-07 ...,   9.26174550e-08\n",
      "    9.99994040e-01   1.35518178e-06]]\n",
      "\n",
      "Minibatch loss at step 1000: 0.986349\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.1%\n",
      "[[  4.78918082e-05   9.93636966e-01   5.70683869e-06 ...,   1.66568043e-05\n",
      "    2.22647086e-05   7.03090482e-05]\n",
      " [  2.69449316e-02   3.32092168e-03   2.21915846e-03 ...,   4.44651669e-04\n",
      "    7.33066201e-02   8.85009706e-01]\n",
      " [  2.86219697e-02   5.45260534e-02   2.03308970e-01 ...,   1.33180320e-02\n",
      "    3.75756919e-01   1.10657252e-01]\n",
      " ..., \n",
      " [  6.87668398e-02   6.44702837e-02   3.94241847e-02 ...,   3.15377377e-02\n",
      "    4.46248502e-01   1.91045418e-01]\n",
      " [  2.10918297e-05   1.83519273e-06   4.08982004e-08 ...,   9.16814827e-07\n",
      "    6.89093795e-05   9.99891877e-01]\n",
      " [  1.07500447e-04   6.62229795e-05   4.90485581e-05 ...,   7.85353041e-05\n",
      "    9.98547852e-01   6.35462988e-04]]\n",
      "\n",
      "Minibatch loss at step 1500: 0.832232\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 82.5%\n",
      "[[  1.13865135e-04   9.92439389e-01   1.64052672e-05 ...,   5.43015813e-05\n",
      "    4.31385670e-05   9.63908824e-05]\n",
      " [  5.18708490e-02   1.00297881e-02   4.50952817e-03 ...,   2.22691870e-03\n",
      "    8.76116976e-02   8.19769979e-01]\n",
      " [  7.00157210e-02   4.91465032e-02   1.25897974e-01 ...,   2.85241976e-02\n",
      "    3.10217410e-01   1.25453994e-01]\n",
      " ..., \n",
      " [  1.16330281e-01   6.17419966e-02   5.49272597e-02 ...,   5.96778989e-02\n",
      "    2.48276457e-01   1.77275956e-01]\n",
      " [  1.24585771e-04   1.51915119e-05   9.33229728e-07 ...,   1.77805305e-06\n",
      "    5.54803410e-04   9.99224544e-01]\n",
      " [  6.55972690e-04   4.41450888e-04   3.66937427e-04 ...,   5.28992387e-04\n",
      "    9.92574811e-01   2.70557124e-03]]\n",
      "\n",
      "Minibatch loss at step 2000: 0.794645\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 82.9%\n",
      "[[  9.17839861e-05   9.90126371e-01   1.51545219e-05 ...,   4.36247356e-05\n",
      "    1.90200699e-05   3.68693982e-05]\n",
      " [  3.01174559e-02   1.41003719e-02   5.68117388e-03 ...,   4.35063243e-03\n",
      "    7.18849674e-02   8.35466444e-01]\n",
      " [  6.36776835e-02   6.39428347e-02   1.66689619e-01 ...,   4.93002757e-02\n",
      "    2.66220272e-01   8.91712382e-02]\n",
      " ..., \n",
      " [  8.13169628e-02   7.88566396e-02   6.31384552e-02 ...,   9.32165831e-02\n",
      "    2.27242902e-01   1.38927966e-01]\n",
      " [  7.08082152e-05   2.02528117e-05   6.55523763e-07 ...,   2.55865893e-06\n",
      "    3.41906445e-04   9.99451578e-01]\n",
      " [  7.83197291e-04   7.99415284e-04   6.47028151e-04 ...,   1.32061483e-03\n",
      "    9.86849189e-01   4.21216618e-03]]\n",
      "\n",
      "Minibatch loss at step 2500: 0.693293\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.1%\n",
      "[[  4.39702708e-05   9.95508671e-01   7.97657594e-06 ...,   3.35419536e-05\n",
      "    1.20043405e-05   1.81840842e-05]\n",
      " [  1.86606199e-02   1.01872422e-02   4.03756974e-03 ...,   2.47102557e-03\n",
      "    5.59228547e-02   8.77743125e-01]\n",
      " [  4.60869633e-02   6.43495172e-02   1.75093248e-01 ...,   4.05361131e-02\n",
      "    3.25062662e-01   1.09110750e-01]\n",
      " ..., \n",
      " [  7.96174034e-02   7.98713118e-02   6.18193559e-02 ...,   7.87691996e-02\n",
      "    2.16105685e-01   1.83961064e-01]\n",
      " [  1.30942091e-04   2.18644691e-05   1.18449054e-06 ...,   2.19224830e-06\n",
      "    9.40810249e-04   9.98708129e-01]\n",
      " [  7.96974287e-04   8.03772127e-04   9.65701940e-04 ...,   1.04593928e-03\n",
      "    9.85907853e-01   5.32448106e-03]]\n",
      "\n",
      "Minibatch loss at step 3000: 0.577506\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.5%\n",
      "[[  4.22607991e-05   9.96755064e-01   6.12865642e-06 ...,   2.38971934e-05\n",
      "    1.08001723e-05   1.42496356e-05]\n",
      " [  1.93708111e-02   1.07532693e-02   4.91385022e-03 ...,   3.53339966e-03\n",
      "    4.76432256e-02   8.79697621e-01]\n",
      " [  7.16237798e-02   5.76611720e-02   1.58255517e-01 ...,   5.75632565e-02\n",
      "    2.58719176e-01   1.20379604e-01]\n",
      " ..., \n",
      " [  7.16000721e-02   8.05513561e-02   5.37351817e-02 ...,   9.54935551e-02\n",
      "    2.07644820e-01   1.56962812e-01]\n",
      " [  2.42087801e-04   1.51558188e-05   9.93961294e-07 ...,   1.14221859e-06\n",
      "    1.88301795e-03   9.97727096e-01]\n",
      " [  3.43560503e-04   4.72568267e-04   5.13722189e-04 ...,   8.44306836e-04\n",
      "    9.92882729e-01   2.14462238e-03]]\n",
      "\n",
      "Minibatch loss at step 3500: 0.815307\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 83.6%\n",
      "[[  2.32753155e-05   9.97759819e-01   2.28534122e-06 ...,   1.72693362e-05\n",
      "    4.80032122e-06   7.18463025e-06]\n",
      " [  7.15877814e-03   6.45750342e-03   2.79872539e-03 ...,   1.47626991e-03\n",
      "    2.42490284e-02   9.41141427e-01]\n",
      " [  6.18273877e-02   6.09516390e-02   1.85247511e-01 ...,   5.93455918e-02\n",
      "    2.58239001e-01   1.38452426e-01]\n",
      " ..., \n",
      " [  6.76639006e-02   7.59410262e-02   6.15881532e-02 ...,   9.00947079e-02\n",
      "    2.04938978e-01   1.79389998e-01]\n",
      " [  1.00849888e-04   5.91397338e-06   2.75762147e-07 ...,   4.36108763e-07\n",
      "    7.28229061e-04   9.99119341e-01]\n",
      " [  5.36628184e-04   5.62723551e-04   5.87887131e-04 ...,   9.85775609e-04\n",
      "    9.89982307e-01   4.00580885e-03]]\n",
      "\n",
      "Minibatch loss at step 4000: 0.538407\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.1%\n",
      "[[  2.07921657e-05   9.98702049e-01   1.27313172e-06 ...,   9.81863923e-06\n",
      "    2.96092458e-06   3.86189140e-06]\n",
      " [  7.01186573e-03   4.06922307e-03   1.92248460e-03 ...,   1.02899165e-03\n",
      "    3.33382823e-02   9.35641527e-01]\n",
      " [  8.09741914e-02   5.86693063e-02   1.31078362e-01 ...,   6.31547943e-02\n",
      "    2.52295643e-01   1.44996494e-01]\n",
      " ..., \n",
      " [  8.84658098e-02   7.89370835e-02   4.93841022e-02 ...,   7.77644664e-02\n",
      "    2.07241178e-01   1.40374914e-01]\n",
      " [  8.27577678e-05   4.81111147e-06   4.64519928e-07 ...,   3.57084161e-07\n",
      "    1.62240281e-03   9.98232186e-01]\n",
      " [  4.45735292e-04   3.16744146e-04   2.29869533e-04 ...,   5.16467146e-04\n",
      "    9.93967891e-01   2.57195532e-03]]\n",
      "\n",
      "Minibatch loss at step 4500: 0.682058\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "[[  1.17867221e-05   9.99286473e-01   9.52906532e-07 ...,   4.95451923e-06\n",
      "    3.05624735e-06   1.72729699e-06]\n",
      " [  6.59399247e-03   4.19439981e-03   2.03556311e-03 ...,   9.84223676e-04\n",
      "    3.11645586e-02   9.38137829e-01]\n",
      " [  7.40740970e-02   6.67068884e-02   1.24672182e-01 ...,   7.69350603e-02\n",
      "    2.63209850e-01   1.48059756e-01]\n",
      " ..., \n",
      " [  7.09547624e-02   7.41008669e-02   5.90898842e-02 ...,   1.01518020e-01\n",
      "    1.91214263e-01   1.52721867e-01]\n",
      " [  7.74744331e-05   5.00581700e-06   4.05259470e-07 ...,   2.06840070e-07\n",
      "    2.33084685e-03   9.97536182e-01]\n",
      " [  3.33092961e-04   3.22359905e-04   3.08371615e-04 ...,   6.52732153e-04\n",
      "    9.94928598e-01   1.77560316e-03]]\n",
      "\n",
      "Minibatch loss at step 5000: 0.619786\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.1%\n",
      "[[  1.97623049e-05   9.98939455e-01   1.72294176e-06 ...,   8.05627042e-06\n",
      "    2.46449827e-06   1.28746751e-06]\n",
      " [  6.32989313e-03   3.79371503e-03   1.66989153e-03 ...,   8.19937268e-04\n",
      "    2.77506094e-02   9.44770932e-01]\n",
      " [  7.83391669e-02   6.34439439e-02   1.33889377e-01 ...,   8.18721130e-02\n",
      "    2.68215626e-01   1.33677021e-01]\n",
      " ..., \n",
      " [  7.17431754e-02   7.09844753e-02   5.87276407e-02 ...,   9.23387706e-02\n",
      "    1.89680442e-01   1.52661532e-01]\n",
      " [  4.25938269e-05   1.20741834e-06   1.76116572e-07 ...,   1.30064592e-07\n",
      "    9.40412458e-04   9.98981535e-01]\n",
      " [  3.14823235e-04   2.47215532e-04   2.60622794e-04 ...,   4.32816334e-04\n",
      "    9.95607674e-01   1.70660485e-03]]\n",
      "\n",
      "Minibatch loss at step 5500: 0.550626\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.4%\n",
      "[[  1.35850605e-05   9.99583542e-01   9.73679334e-07 ...,   3.52280085e-06\n",
      "    2.40830332e-06   8.40839789e-07]\n",
      " [  5.78272296e-03   3.05787753e-03   1.62303634e-03 ...,   6.82824175e-04\n",
      "    3.73188555e-02   9.38548982e-01]\n",
      " [  8.16713274e-02   5.96628450e-02   1.24037206e-01 ...,   7.72043988e-02\n",
      "    2.76315480e-01   1.30794987e-01]\n",
      " ..., \n",
      " [  6.97653592e-02   5.85885867e-02   5.18679060e-02 ...,   7.95888305e-02\n",
      "    2.61940897e-01   1.64104909e-01]\n",
      " [  2.83538739e-05   1.16741262e-06   1.76657323e-07 ...,   7.01381353e-08\n",
      "    1.81967008e-03   9.98126924e-01]\n",
      " [  3.29295435e-04   1.09132867e-04   1.11897454e-04 ...,   2.40804206e-04\n",
      "    9.96509373e-01   1.46671257e-03]]\n",
      "\n",
      "Minibatch loss at step 6000: 0.466829\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.7%\n",
      "[[  1.22525698e-05   9.99506593e-01   1.14651368e-06 ...,   3.09477218e-06\n",
      "    3.58935836e-06   9.30437523e-07]\n",
      " [  4.22802102e-03   2.27772258e-03   1.36821740e-03 ...,   6.15804747e-04\n",
      "    3.91011462e-02   9.40466642e-01]\n",
      " [  8.10752884e-02   5.77933900e-02   1.05756007e-01 ...,   8.08358863e-02\n",
      "    2.61333615e-01   1.40661746e-01]\n",
      " ..., \n",
      " [  6.36020154e-02   6.50803223e-02   4.77688424e-02 ...,   9.50434208e-02\n",
      "    2.28816181e-01   1.53233960e-01]\n",
      " [  4.02975384e-05   1.39416238e-06   2.69802030e-07 ...,   1.30145381e-07\n",
      "    3.27393366e-03   9.96649206e-01]\n",
      " [  2.05997756e-04   1.27524705e-04   1.05499610e-04 ...,   2.23919575e-04\n",
      "    9.97337878e-01   1.02644542e-03]]\n",
      "\n",
      "Minibatch loss at step 6500: 0.666726\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.9%\n",
      "[[  1.89167931e-05   9.98935401e-01   1.31624631e-06 ...,   4.24351720e-06\n",
      "    2.18025070e-06   7.13458292e-07]\n",
      " [  5.44543192e-03   2.87813041e-03   1.51392387e-03 ...,   6.16962672e-04\n",
      "    2.70988438e-02   9.51900065e-01]\n",
      " [  9.39403921e-02   6.46185204e-02   1.16315030e-01 ...,   8.59645829e-02\n",
      "    2.47671500e-01   1.19731516e-01]\n",
      " ..., \n",
      " [  7.29039833e-02   7.22429752e-02   6.28934726e-02 ...,   9.30203274e-02\n",
      "    2.16073260e-01   1.24657393e-01]\n",
      " [  3.30544608e-05   6.91502919e-07   1.54588861e-07 ...,   5.51713626e-08\n",
      "    8.51442106e-04   9.99098420e-01]\n",
      " [  2.70242075e-04   1.78684539e-04   2.26052958e-04 ...,   3.41531500e-04\n",
      "    9.97145712e-01   8.24512099e-04]]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"\\nMinibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(valid_prediction.eval())\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
